{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5541a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# known params\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "#inputs\n",
    "X = torch.arange(start, end, step).unsqueeze(1)\n",
    "#outputs\n",
    "y = weight * X + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X) == len(y) == 50, splitting data 80/20 yeilds 40/10 samples for training and testing respectively\n",
    "train_split = 40\n",
    "X_train, y_train = X[:train_split],y[:train_split]\n",
    "X_test, y_test = X[train_split:],y[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize both training and test data!\n",
    "def plot_predictions(train_data=X_train,\n",
    "                    train_labels=y_train,\n",
    "                    test_data=X_test,\n",
    "                    test_labels=y_test,\n",
    "                    predictions=None):\n",
    "    \n",
    "    plt.figure(figsize=(10,7))\n",
    "\n",
    "    # plot training data\n",
    "    plt.scatter(train_data, train_labels, c='b', s=4, label='Training data')\n",
    "\n",
    "    # plot testing data\n",
    "    plt.scatter(test_data, test_labels, c='g', s=4, label='Testing data')\n",
    "\n",
    "    # are there predictions?\n",
    "    if predictions is not None:\n",
    "        plt.scatter(test_data, predictions, c='r', s=4, label='Predictions')\n",
    "\n",
    "    # legend\n",
    "    plt.legend(prop={'size':14})\n",
    "\n",
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now actually build the model to try and predict the green values shown above.\n",
    "# we will use gradient descent and backpropagation\n",
    "\n",
    "class LinearRegressionModel(nn.Module): # almost everything from PyTorch inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1,                  # start w radom value and adjust\n",
    "                                                requires_grad=True, # keep track of grad to update with grad desc\n",
    "                                                dtype=torch.float)) # pytorch dtype\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.randn(1, \n",
    "                                             requires_grad=True, \n",
    "                                             dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor: # x is training data\n",
    "        return self.weights * x + self.bias # linear regression formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0465877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create an instance of the model\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# check out params. We want to move these values closer to the actual weight and bias values\n",
    "model_0.state_dict(), weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with current model\n",
    "with torch.inference_mode(): \n",
    "    y_preds = model_0(X_test)\n",
    "    \n",
    "plot_predictions(predictions=y_preds) # very bad :( "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f414f",
   "metadata": {},
   "source": [
    "### Steps to building a training loop\n",
    "0. Loop through the data\n",
    "1. Forward pass to make predictions on data - aka forward propagation\n",
    "2. Calculate the loss (compare forward pass predictions to ground truth labels)\n",
    "3. Optimizer zero grad\n",
    "4. Loss backward - move backwards throguh the network to calculate the gradients of each parameter wrt the loss - aka backpropagation\n",
    "5. Optimizer step - use the optimizer to adjust the model parameters to try and improve the loss - aka gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss() # MAE / L1\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), # params to optimize\n",
    "                           lr=0.01) # learning rate\n",
    "\n",
    "# an epoch is one loop through the data\n",
    "epochs = 201\n",
    "\n",
    "# track different values\n",
    "epoch_count = []\n",
    "loss_values = []\n",
    "test_loss_values = []\n",
    "\n",
    "# step 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # set the model to training mode\n",
    "    model_0.train() \n",
    "    \n",
    "    # 1. forward pass\n",
    "    y_pred = model_0(X_train)\n",
    "    \n",
    "    # 2. calculate loss\n",
    "    loss = loss_fn(y_pred, y_train) # prediction first, truths second\n",
    "    \n",
    "    # 3. zero the gradients of the optimizer (they accumulate by default)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. backpropagation to calculate gradients\n",
    "    loss.backward() # watch 3b1b backpropagation\n",
    "    \n",
    "    # 5. gradient descent\n",
    "    optimizer.step() \n",
    "    \n",
    "############################################################################################################################\n",
    "    \n",
    "    # set the model to testing mode\n",
    "    model_0.eval()\n",
    "    \n",
    "    # turn off gradient tracking (optimization), better vesion of torch.no_grad()\n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        # 1. forward pass\n",
    "        test_pred = model_0(X_test)\n",
    "        \n",
    "        # 2. calculate loss\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "    \n",
    "    # keep track of values\n",
    "    if epoch % 10 == 0:\n",
    "        epoch_count.append(epoch)\n",
    "        loss_values.append(loss)\n",
    "        test_loss_values.append(test_loss)\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss} | Test Loss: {test_loss}\")\n",
    "        print(f\"Current State: {model_0.state_dict()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode(): \n",
    "    y_preds_new = model_0(X_test)\n",
    "\n",
    "plot_predictions(predictions=y_preds_new) # better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values_np = []\n",
    "for value in range(len(loss_values)):\n",
    "    loss_values_np.append(loss_values[value].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b332fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also plot the loss curves\n",
    "plt.plot(epoch_count, loss_values_np, label=\"Training Loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test Loss\")\n",
    "plt.title(\"Training and Test Loss Curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "from pathlib import Path\n",
    "\n",
    "# create model directory\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create model save path \n",
    "MODEL_NAME = 'pytorch_tutorial_model_0.pth'\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# save the state_dict()\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_0.state_dict(),\n",
    "          f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ada74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "# since the models state_dict() was saved, we can create a new instance of the model and load the saved state_dict()\n",
    "\n",
    "loaded_model_0 = LinearRegressionModel()\n",
    "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bc5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0451f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some predictions with the loaded model\n",
    "loaded_model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    loaded_model_preds = loaded_model_0(X_test)\n",
    "\n",
    "loaded_model_preds == test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e94c444",
   "metadata": {},
   "source": [
    "## Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639643a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create target weight and bias value along with dummy data\n",
    "weight = 0.5\n",
    "bias = 0.8\n",
    "\n",
    "X = torch.arange(0, 1, 0.01).unsqueeze(1)\n",
    "y = weight * X + bias\n",
    "\n",
    "split = int(0.8 * len(X))\n",
    "\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]\n",
    "\n",
    "# create a function to plot the data\n",
    "def plot_predictions(predictions=None):\n",
    "    \n",
    "    plt.figure(figsize=(10,7))\n",
    "\n",
    "    plt.scatter(X_train, y_train, c='b', s=4, label='Training Data')\n",
    "    plt.scatter(X_test, y_test, c='r', s=4, label='Testing Data')\n",
    "    \n",
    "    if predictions is not None:\n",
    "        plt.scatter(X_test, predictions, c='g', s=4, label='Predictions')\n",
    "        \n",
    "    plt.legend()\n",
    "    \n",
    "# create the regression class\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        torch.manual_seed(0)\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_features=1, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "    \n",
    "# instantiate the model\n",
    "model_1 = LinearRegression()\n",
    "\n",
    "# check the starting point\n",
    "with torch.inference_mode():\n",
    "    y_pred = model_1(X_test)\n",
    "plot_predictions(y_pred)\n",
    "\n",
    "# set up loss function and optimizer for training \n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.01)\n",
    "\n",
    "# initialize lists for analysis\n",
    "epoch_count = []\n",
    "loss_value = []\n",
    "test_loss_value = []\n",
    "\n",
    "# training loop\n",
    "def train(model, epochs):\n",
    "\n",
    "    for epoch in range(epochs + 1):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            test_pred = model(X_test)\n",
    "            test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} | Loss: {loss} | Test Loss: {test_loss}\")\n",
    "            print(model.state_dict())\n",
    "            print()\n",
    "            \n",
    "            epoch_count.append(epoch)\n",
    "            loss_value.append(loss)\n",
    "            test_loss_value.append(test_loss)\n",
    "\n",
    "train(model_1, 200)\n",
    "\n",
    "# re-plot\n",
    "with torch.inference_mode(): \n",
    "    y_new = model_1(X_test)\n",
    "plot_predictions(y_new)\n",
    "\n",
    "# plot the cost curve\n",
    "plt.figure()\n",
    "loss_value_np = [loss_value[i].detach().numpy() for i in range(len(loss_value))]\n",
    "plt.plot(epoch_count, loss_value_np, c='y',label='Loss')\n",
    "plt.plot(epoch_count, test_loss_value, c='b',label='Test Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e762a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
